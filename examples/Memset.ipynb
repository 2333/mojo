{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Memset in Mojo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will implement a memset version optimized for small sizes\n",
    "using Mojo's autotuning feature.\n",
    "\n",
    "The idea behind the implementation is based on Nadav Rotem's work [[1](https://github.com/nadavrot/memset_benchmark)], and is also well-described in [[2](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/4f7c3da72d557ed418828823a8e59942859d677f.pdf)].\n",
    "\n",
    "Below we try to briefly summarize the approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level overview\n",
    "\n",
    "For the best memset performance we want to use the widest possible register\n",
    "width for the memory access. For instance, if we want to store 19 bytes, we\n",
    "want to use vector width 16 and use two overlapping stores. To store 9 bytes,\n",
    "we would want to use two 8-byte stores.\n",
    "\n",
    "However, before we get to actually doing stores, we need to perform size\n",
    "checks to make sure that we're in the right range. I.e. we want to use 8\n",
    "bytes stores for sizes 8-16, 16 bytes stores for sizes 16-32, etc.\n",
    "\n",
    "The order in which we do the size checks significantly affects performance\n",
    "and ideally we would like to run as few checks as possible for the sizes\n",
    "that occur most often. I.e. if most of the sizes we see are 16-32, then we\n",
    "want to first check if it's within that range before we check if it's in\n",
    "8-16 or some other range.\n",
    "\n",
    "This results in a number of different comparison \"trees\" that can be used to\n",
    "perform the size checks, and in this tutorial we use Mojo's autotuning to pick\n",
    "the most optimal one given the distribution of input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "We will start as we always start - with imports and type aliases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Assert import assert_param\n",
    "from Autotune import autotune_fork, search\n",
    "from DType import DType\n",
    "from IO import print, _printf, put\n",
    "from List import VariadicList\n",
    "from Math import min, max\n",
    "from Memory import _malloc, _free\n",
    "from OS import getenv\n",
    "from Pointer import DTypePointer, Pointer\n",
    "from Range import range\n",
    "from SIMD import SIMD\n",
    "from Sort import sort\n",
    "from String import StringRef\n",
    "from TargetInfo import sizeof\n",
    "from Time import now\n",
    "from Vector import DynamicVector\n",
    "\n",
    "alias UI8 = DType.ui8\n",
    "alias BufferPtrType = DTypePointer[UI8]\n",
    "alias ValueType = SIMD[UI8, 1]\n",
    "alias NoneType = __mlir_type.`!lit.none`\n",
    "\n",
    "alias memset_fn_type = __mlir_type[\n",
    "    `(`, BufferPtrType, `, `, ValueType, `, `, Int, `) -> `, NoneType\n",
    "]\n",
    "alias memset_fn_sig_type = fn(BufferPtrType, ValueType, Int) -> None\n",
    "alias memset_fn_ptr_type = __mlir_type[\n",
    "    `!pop.pointer<`,\n",
    "    memset_fn_sig_type,\n",
    "    `>`\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add some auxiliary function. We will use them to benchmark various\n",
    "memset implementations and visualize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn optimization_barrier(ptr: BufferPtrType):\n",
    "    __mlir_op.`pop.inline_asm`[\n",
    "        _type:None,\n",
    "        assembly:(\"\").value,\n",
    "        constraints:(\"r,~{memory}\").value,\n",
    "        hasSideEffects : __mlir_attr.unit,\n",
    "    ](ptr.address)\n",
    "\n",
    "\n",
    "fn alloc_buffer(size: Int) -> BufferPtrType:\n",
    "    let data_mem = _malloc[ValueType](sizeof[ValueType]() * size)\n",
    "    return DTypePointer[UI8.value](data_mem.address)\n",
    "\n",
    "\n",
    "fn free_buffer(ptr: BufferPtrType):\n",
    "    _free[ValueType](ptr.as_scalar_pointer())\n",
    "\n",
    "\n",
    "fn measure_time(\n",
    "    func: memset_fn_type, size: Int, ITERS: Int, SAMPLES: Int\n",
    ") -> Int:\n",
    "    alias alloc_size = 1024 * 1024\n",
    "    let ptr = alloc_buffer(alloc_size)\n",
    "\n",
    "    var samples = DynamicVector[Int](SAMPLES)\n",
    "\n",
    "    for sample in range(SAMPLES):\n",
    "        let tic = now()\n",
    "        for iter in range(ITERS):\n",
    "            # Offset pointer to shake up cache a bit\n",
    "            let offset_ptr = ptr.offset((iter * 128) & 1024)\n",
    "\n",
    "            # Just in case compiler will try to outsmart us and avoid repeating\n",
    "            # memset, change the value we're filling with\n",
    "            let v = ValueType(iter&255)\n",
    "\n",
    "            # Actually call the memset function\n",
    "            __mlir_op.`pop.call_indirect`[_type:NoneType](\n",
    "                func, offset_ptr, v.value, size\n",
    "            )\n",
    "\n",
    "            # Insert optimization barriers to prevent compiler from optimizing\n",
    "            # this loop away\n",
    "            optimization_barrier(ptr)\n",
    "            optimization_barrier(offset_ptr)\n",
    "\n",
    "        let toc = now()\n",
    "        samples.push_back(toc - tic)\n",
    "\n",
    "    # Find median across the samples\n",
    "    sort(samples)\n",
    "    let result = samples[SAMPLES // 2]\n",
    "\n",
    "    samples.__del__()\n",
    "    free_buffer(ptr)\n",
    "    return result\n",
    "\n",
    "\n",
    "fn visualize_result(size: Int, result: Int):\n",
    "    _printf(\"Size: \")\n",
    "    if size < 10:\n",
    "        _printf(\" \")\n",
    "    put(size)\n",
    "    _printf(\"   |\")\n",
    "    for _ in range(result // 10000):\n",
    "        _printf(\"*\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "fn benchmark(func: memset_fn_type, title: StringRef):\n",
    "    print(\"\\n================\")\n",
    "    print(title)\n",
    "    print(\"----------------\\n\")\n",
    "\n",
    "    alias warmup_iterations = 100\n",
    "    alias benchmark_iterations = 100000\n",
    "    alias benchmark_samples = 5\n",
    "\n",
    "    for size in range(35):\n",
    "        # Warmup\n",
    "        _ = measure_time(func, size, warmup_iterations, 1)\n",
    "\n",
    "        # Actual run\n",
    "        let result = measure_time(\n",
    "            func, size, benchmark_iterations, benchmark_samples\n",
    "        )\n",
    "\n",
    "        visualize_result(size, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducing results from the paper\n",
    "\n",
    "Let's implement a memset version from the paper in Mojo and compare it against\n",
    "the system memset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@always_inline\n",
    "fn overlapped_store[\n",
    "    width: Int\n",
    "](ptr: BufferPtrType, value: ValueType, count: Int):\n",
    "    let v = SIMD.splat[UI8, width](value)\n",
    "    ptr.simd_store[width](v)\n",
    "    ptr.simd_store[width](count - width, v)\n",
    "\n",
    "\n",
    "fn memset_manual(ptr: BufferPtrType, value: ValueType, count: Int):\n",
    "    if count < 32:\n",
    "        if count < 5:\n",
    "            if count == 0:\n",
    "                return\n",
    "            # 0 < count <= 4\n",
    "            ptr.store(0, value)\n",
    "            ptr.store(count - 1, value)\n",
    "            if count <= 2:\n",
    "                return\n",
    "            ptr.store(1, value)\n",
    "            ptr.store(count - 2, value)\n",
    "            return\n",
    "\n",
    "        if count <= 16:\n",
    "            if count >= 8:\n",
    "                # 8 <= count < 16\n",
    "                overlapped_store[8](ptr, value, count)\n",
    "                return\n",
    "            # 4 < count < 8\n",
    "            overlapped_store[4](ptr, value, count)\n",
    "            return\n",
    "\n",
    "        # 16 <= count < 32\n",
    "        overlapped_store[16](ptr, value, count)\n",
    "    else:\n",
    "        # 32 < count\n",
    "        memset_system(ptr, value, count)\n",
    "\n",
    "\n",
    "fn memset_system(ptr: BufferPtrType, value: ValueType, count: Int):\n",
    "    __mlir_op.`pop.memset`(ptr.address, value.value, count.__as_mlir_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| CHECK: Manual memset\n",
    "#| CHECK: System memset\n",
    "let fptr_manual = __mlir_op.`kgen.addressof`[\n",
    "    _type:memset_fn_type,\n",
    "    callee:memset_manual,\n",
    "    paramDecls : __mlir_attr.`#kgen<param.decls[]>`,\n",
    "]()\n",
    "let fptr_system = __mlir_op.`kgen.addressof`[\n",
    "    _type:memset_fn_type,\n",
    "    callee:memset_system,\n",
    "    paramDecls : __mlir_attr.`#kgen<param.decls[]>`,\n",
    "]()\n",
    "benchmark(fptr_manual, \"Manual memset\")\n",
    "benchmark(fptr_system, \"System memset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweaking the implementation for different sizes\n",
    "\n",
    "We can see that it's already much faster for small sizes.\n",
    "That version was specifically optimized for a certain input size distribution,\n",
    "e.g. we can see that sizes 8-16 and 0-4 work fastest.\n",
    "\n",
    "But what if in **our use case** the distribution is different? Let's imagine that\n",
    "in our case the most common sizes are 16-32 - is this version the most optimal\n",
    "version we can use then? The answer is obviously \"no\", and we can easily tweak\n",
    "the implementation to work better for these sizes - we just need to move the\n",
    "corresponding check closer to the beginning of the function. E.g. like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn memset_manual_2(ptr: BufferPtrType, value: ValueType, count: Int):\n",
    "    if count < 32:\n",
    "        if count >= 16:\n",
    "            # 16 <= count < 32\n",
    "            overlapped_store[16](ptr, value, count)\n",
    "            return\n",
    "\n",
    "        if count < 5:\n",
    "            if count == 0:\n",
    "                return\n",
    "            # 0 < count <= 4\n",
    "            ptr.store(0, value)\n",
    "            ptr.store(count - 1, value)\n",
    "            if count <= 2:\n",
    "                return\n",
    "            ptr.store(1, value)\n",
    "            ptr.store(count - 2, value)\n",
    "            return\n",
    "\n",
    "        if count >= 8:\n",
    "            # 8 <= count < 16\n",
    "            overlapped_store[8](ptr, value, count)\n",
    "            return\n",
    "        # 4 < count < 8\n",
    "        overlapped_store[4](ptr, value, count)\n",
    "\n",
    "    else:\n",
    "        # 32 < count\n",
    "        memset_system(ptr, value, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the performance of this version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| CHECK: Manual memset v2\n",
    "let fptr_manual_2 = __mlir_op.`kgen.addressof`[\n",
    "    _type:memset_fn_type,\n",
    "    callee:memset_manual_2,\n",
    "    paramDecls : __mlir_attr.`#kgen<param.decls[]>`,\n",
    "]()\n",
    "benchmark(fptr_manual_2, \"Manual memset v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is now much better on the 16-32 sizes!\n",
    "\n",
    "The problem is that we had to manually re-write the code. Wouldn't it be nice\n",
    "if it was done automatically?\n",
    "\n",
    "In Mojo this is possible (and quite easy) - we can generate multiple\n",
    "implementations and let the compiler pick the fastest one for us evaluating\n",
    "them on sizes we want!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mojo implementation\n",
    "\n",
    "Let's dive into that.\n",
    "\n",
    "The first thing we need to do is to generate all possible candidates. To do\n",
    "that we will need to iteratively generate size checks to understand what size\n",
    "for the overlapping store we can use. Once we localize the size interval, we\n",
    "just call the overlapping store of the corresponding size.\n",
    "\n",
    "To express this we will implement an adaptive function `memset_impl_layer` two\n",
    "parameters designating the current interval of possible size values. When we\n",
    "generate a new size check, we split that interval into two parts and\n",
    "recursively call the same functions on those two parts. Once we reach the\n",
    "minimal intervals, we will call the corresponding overlapped_store function.\n",
    "\n",
    "This first implementation covers minimal interval cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@adaptive\n",
    "@always_inline\n",
    "fn memset_impl_layer[\n",
    "    lower: Int, upper: Int\n",
    "](ptr: BufferPtrType, value: ValueType, count: Int):\n",
    "    @parameter\n",
    "    if (lower == -100) & (upper == 0):\n",
    "        pass\n",
    "    elif (lower == 0) & (upper == 4):\n",
    "        ptr.store(0, value)\n",
    "        ptr.store(count - 1, value)\n",
    "        if count <= 2:\n",
    "            return\n",
    "        ptr.store(1, value)\n",
    "        ptr.store(count - 2, value)\n",
    "    elif (lower == 4) & (upper == 8):\n",
    "        overlapped_store[4](ptr, value, count)\n",
    "    elif (lower == 8) & (upper == 16):\n",
    "        overlapped_store[8](ptr, value, count)\n",
    "    elif (lower == 16) & (upper == 32):\n",
    "        overlapped_store[16](ptr, value, count)\n",
    "    elif (lower == 32) & (upper == 100):\n",
    "        memset_system(ptr, value, count)\n",
    "    else:\n",
    "        assert_param[False]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now add an implementation for the other case, where we need to generate a\n",
    "size check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@adaptive\n",
    "@always_inline\n",
    "fn memset_impl_layer[\n",
    "    lower: Int, upper: Int\n",
    "](ptr: BufferPtrType, value: ValueType, count: Int):\n",
    "    alias cur: Int\n",
    "    autotune_fork[Int, 0, 4, 8, 16, 32 -> cur]()\n",
    "\n",
    "    assert_param[cur > lower]()\n",
    "    assert_param[cur < upper]()\n",
    "\n",
    "    if count > cur:\n",
    "        memset_impl_layer[max(cur, lower), upper](ptr, value, count)\n",
    "    else:\n",
    "        memset_impl_layer[lower, min(cur, upper)](ptr, value, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use 'autotune_fork' to generate all possible at that point checks.\n",
    "\n",
    "We will discard values beyond the current interval, and for the values within\n",
    "we will recursively call this function on the interval splits.\n",
    "\n",
    "This is sufficient to generate multiple correct versions of memset, but to\n",
    "achieve the best performance we need to take into account one more factor: when\n",
    "we're dealing with such small sizes, even the code location matters a lot. E.g.\n",
    "if we swap Then and Else branches and invert the condition, we might get a\n",
    "different performance of the final function.\n",
    "\n",
    "To account for that, let's add one more implementation of our function, but now\n",
    "with branches swapped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@adaptive\n",
    "@always_inline\n",
    "fn memset_impl_layer[\n",
    "    lower: Int, upper: Int\n",
    "](ptr: BufferPtrType, value: ValueType, count: Int):\n",
    "    alias cur: Int\n",
    "    autotune_fork[Int, 0, 4, 8, 16, 32 -> cur]()\n",
    "\n",
    "    assert_param[cur > lower]()\n",
    "    assert_param[cur < upper]()\n",
    "\n",
    "    if count <= cur:\n",
    "        memset_impl_layer[lower, min(cur, upper)](ptr, value, count)\n",
    "    else:\n",
    "        memset_impl_layer[max(cur, lower), upper](ptr, value, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined building blocks for our implementation, now we need to add a top\n",
    "level entry-point that will kick off the recursion we've just defined.\n",
    "\n",
    "We will simply call our function with [-100,100] interval - -100 and 100 simply\n",
    "designate that no checks have been performed yet. This interval will be refined\n",
    "as we generate more and more check until we have enough to emit actual stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@adaptive\n",
    "fn memset_autotune_impl(ptr: BufferPtrType, value: ValueType, count: Int):\n",
    "    memset_impl_layer[-100, 100](ptr, value, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we're done with our memset implementation, now we just need to plug it to\n",
    "autotuning infrastructure to let the Mojo compiler do the search and pick the\n",
    "best implementation.\n",
    "\n",
    "To do that, we need to define an evaluator - this is a function that will take\n",
    "an array of function pointers to all implementations of our function and will\n",
    "need to return an index of the best candidate.\n",
    "\n",
    "There are no limitations in how this function can be implemented - it can\n",
    "return the first or a random candidate, or it can actually benchmark all of\n",
    "them and pick the fastest - this is what we're going to do for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn memset_evaluator(funcs: Pointer[memset_fn_sig_type], size: Int) -> Int:\n",
    "\n",
    "    print(\"memset_evaluator, number of candidates: \")\n",
    "    print(size)\n",
    "    let eval_begin: Int = now()\n",
    "\n",
    "    # This size is picked at random, in real code we could use a real size\n",
    "    # distribution here.\n",
    "    print(\"Optimizing for size: \")\n",
    "    let size_to_optimize_for = 17\n",
    "    print(size_to_optimize_for)\n",
    "\n",
    "    var best_idx: Int = -1\n",
    "    var best_time: Int = -1\n",
    "    var funcs_ptr = funcs.bitcast[memset_fn_type]()\n",
    "\n",
    "    alias eval_iterations = 10000\n",
    "    alias eval_samples = 10\n",
    "\n",
    "    # Find the function that's the fastest on the size we're optimizing for\n",
    "    for f_idx in range(size):\n",
    "        let func = funcs_ptr.load(f_idx)\n",
    "        let cur_time = measure_time(\n",
    "            func, size_to_optimize_for, eval_iterations, eval_samples\n",
    "        )\n",
    "        if best_idx < 0:\n",
    "            best_idx = f_idx\n",
    "            best_time = cur_time\n",
    "        if best_time > cur_time:\n",
    "            best_idx = f_idx\n",
    "            best_time = cur_time\n",
    "\n",
    "    let eval_end: Int = now()\n",
    "    print(\"Time spent in memset_evaluator, ms: \")\n",
    "    print((eval_end - eval_begin) // 1000000)\n",
    "\n",
    "    return best_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluator is ready, the last brush stroke is to add a function that will\n",
    "call the best candidate.\n",
    "\n",
    "The search will be performed at compile time, and at runtime we will go\n",
    "directly to the best implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn memset_autotune(ptr: BufferPtrType, value: ValueType, count: Int):\n",
    "    # Get the set of all candidates\n",
    "    alias candidates = memset_autotune_impl.__adaptive_set\n",
    "\n",
    "    # Use the evaluator to select the best candidate.\n",
    "    alias best_impl: memset_fn_sig_type\n",
    "    search[memset_fn_sig_type, VariadicList(candidates), memset_evaluator -> best_impl]()\n",
    "\n",
    "    # Run the best candidate\n",
    "    return best_impl(ptr, value, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to benchmark our function, let's see how its performance looks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| CHECK: Mojo autotune memset\n",
    "let fptr_autotune = __mlir_op.`kgen.addressof`[\n",
    "    _type:memset_fn_type,\n",
    "    callee:memset_autotune,\n",
    "    paramDecls : __mlir_attr.`#kgen<param.decls[]>`,\n",
    "]()\n",
    "benchmark(fptr_autotune, \"Mojo autotune memset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mojo",
   "language": "mojo",
   "name": "mojo-jupyter-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "mojo"
   },
   "file_extension": ".mojo",
   "mimetype": "text/x-mojo",
   "name": "mojo"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
