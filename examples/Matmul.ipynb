{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes how to write a matrix multiplication (matmul) algorithm in Mojo. We will start with a pure Python implementation, transition to a naive implementation that is essentially a copy of the Python one, then add types, then continue the optimizations by vectorizing, tiling, and parallelizing the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define matrix multiplication. Given two dense matrices $A$ and $B$ of dimensions $M\\times K$ and $K\\times N$ respectively, we want to compute their dot product $C = A . B$ (also known as matmul). The dot product $C += A . B$ is defined by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$C_{i,j} += \\sum_{k \\in [0 \\cdots K)} A_{i,k} B_{k,j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format of this demo is to start with an implementation which is identical to that of Python (effectively renaming the file extension), then look at how adding types to the implementation helps performance before extending the implementation by leveraging the vectorization and parallelization capabilities available on modern hardware. Throughout the execution, we report the GFlops achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first implement matmul in Python directly from the definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%python\n",
    "def matmul_python(C, A, B):\n",
    "    for m in range(C.rows):\n",
    "        for n in range(C.cols):\n",
    "            for k in range(A.cols):\n",
    "                C[m, n] += A[m, k] * B[k, n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's benchmark our implementation using 128 by 128 square matrices and report the achieved GFLops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import numpy as np\n",
    "from timeit import timeit\n",
    "\n",
    "class Matrix:\n",
    "    def __init__(self, value, rows, cols):\n",
    "        self.value = value\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        \n",
    "    def __getitem__(self, idxs):\n",
    "        return self.value[idxs[0]][idxs[1]]\n",
    "    \n",
    "    def __setitem__(self, idxs, value):\n",
    "        self.value[idxs[0]][idxs[1]] = value\n",
    "\n",
    "def benchmark_matmul_python(M, N, K):\n",
    "    A = Matrix(list(np.random.rand(M, K)), M, K)\n",
    "    B = Matrix(list(np.random.rand(K, N)), K, N)\n",
    "    C = Matrix(list(np.zeros((M, N))), M, N)\n",
    "    secs = timeit(lambda: matmul_python(C, A, B), number=2)/2\n",
    "    print(((2*M*N*K)/secs) / 1e9, \"GFLOP/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005480328057626661 GFLOP/s\n"
     ]
    }
   ],
   "source": [
    "# %python # TODO: delete this once we switch to REPL in notebook testing (#12719).\n",
    "benchmark_matmul_python(128, 128, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Python implementation to Mojo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Mojo is as simple as Python. First, let's include that modules from the Mojo stdlib that we are going to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Benchmark import Benchmark\n",
    "from DType import DType\n",
    "from IO import print, print_no_newline, _printf\n",
    "from Intrinsics import strided_load\n",
    "from List import VariadicList, VariadicListMem\n",
    "from Math import div_ceil\n",
    "from Memory import memset_zero\n",
    "from Object import object, Attr\n",
    "from Pointer import DTypePointer\n",
    "from Random import rand, random_f64\n",
    "from Range import range\n",
    "from SIMD import SIMD, F64, F32\n",
    "from TargetInfo import dtype_sizeof\n",
    "from Complex import ComplexSIMD as ComplexGenericSIMD\n",
    "\n",
    "alias python_gflops = F64(0.005480328057626661)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can copy and paste our Python code. Mojo is a superset of Python, so the same Python code will run as Mojo code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This exactly the same Python implementation, \n",
    "# but is infact Mojo code!\n",
    "def matmul_untyped(C, A, B):\n",
    "    for m in range(C.rows):\n",
    "        for n in range(C.cols):\n",
    "            for k in range(A.cols):\n",
    "                C[m, n] += A[m, k] * B[k, n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then benchmark the implementation. As before we use a 128 by 128 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def matrix_getitem(self, i) -> object:\n",
    "    return self.value[i]\n",
    "\n",
    "\n",
    "def matrix_setitem(self, i, value) -> object:\n",
    "    self.value[i] = value\n",
    "    return None\n",
    "\n",
    "\n",
    "def matrix_append(self, value) -> object:\n",
    "    self.value.append(value)\n",
    "    return None\n",
    "\n",
    "\n",
    "def matrix_init(rows: Int, cols: Int) -> object:\n",
    "    value = object([])\n",
    "    let getitem: object = __mlir_op.`kgen.addressof`[ _type : object.binary_function, callee:matrix_getitem, paramDecls : __mlir_attr.`#kgen<param.decls[]>`, ]()\n",
    "    let setitem: object = __mlir_op.`kgen.addressof`[ _type : object.ternary_function, callee:matrix_setitem, paramDecls : __mlir_attr.`#kgen<param.decls[]>`, ]()\n",
    "    let append: object = __mlir_op.`kgen.addressof`[ _type : object.binary_function, callee:matrix_append, paramDecls : __mlir_attr.`#kgen<param.decls[]>`, ]()\n",
    "    return object(\n",
    "        Attr(\"value\", value), Attr(\"__getitem__\", getitem), Attr(\"__setitem__\", setitem), \n",
    "        Attr(\"rows\", rows), Attr(\"cols\", cols), Attr(\"append\", append),\n",
    "    )\n",
    "\n",
    "def benchmark_matmul_untyped(M: Int, N: Int, K: Int):\n",
    "    C = matrix_init(M, N)\n",
    "    A = matrix_init(M, K)\n",
    "    B = matrix_init(K, N)\n",
    "    for i in range(M):\n",
    "        c = object([])\n",
    "        b = object([])\n",
    "        a = object([])\n",
    "        for j in range(N):\n",
    "            c.append(0.0)\n",
    "            b.append(random_f64(-5, 5))\n",
    "            a.append(random_f64(-5, 5))\n",
    "        C.append(c)\n",
    "        B.append(b)\n",
    "        A.append(a)\n",
    "\n",
    "    fn test_fn():\n",
    "        try:\n",
    "            matmul_untyped(C, A, B)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    let secs = F64(Benchmark().run[test_fn]()) / 1_000_000_000\n",
    "    let gflops = ((2*M*N*K)/secs) / 1e9\n",
    "    let speedup : F64 = gflops / python_gflops\n",
    "    print_no_newline(gflops, \"GFLOP/s, a \")\n",
    "    _printf(\"%0.2f\", speedup.value)\n",
    "    print(\"x speedup over Python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.047082 GFLOP/s, a 8.59x speedup over Python\n"
     ]
    }
   ],
   "source": [
    " #| CHECK: GFLOP/s\n",
    "benchmark_matmul_untyped(128, 128, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the huge speedup with no effort that we have gotten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding types to the Python implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above program, while achieving better performance than Python, is still not the best we can get from Mojo. If we tell Mojo the types of the inputs, it can optimize much of the code away and reduce dispatching costs (unlike Python, which only uses types for type checking, Mojo exploits type info for performance optimizations as well)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do that, let's first define a `Matrix` struct. The `Matrix` struct contains a data pointer along with size fields. While the `Matrix` struct can be parametrized on any data type, here we set the data type to be f32 for conciseness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "struct Matrix:\n",
    "    var data: DTypePointer[DType.f32]\n",
    "    var rows: Int\n",
    "    var cols: Int\n",
    "\n",
    "    fn __init__(self&, rows: Int, cols: Int):\n",
    "        self.data = DTypePointer[DType.f32].alloc(rows * cols)\n",
    "        rand(self.data, rows*cols)\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "\n",
    "    fn __del__(owned self):\n",
    "        self.data.free()\n",
    "\n",
    "    fn zero(self&):\n",
    "        memset_zero(self.data, self.rows * self.cols * dtype_sizeof[DType.f32]())\n",
    "\n",
    "    @always_inline\n",
    "    fn __getitem__(self, y: Int, x: Int) -> F32:\n",
    "        return self.load[1](y, x)\n",
    "\n",
    "    @always_inline\n",
    "    fn load[nelts:Int](self, y: Int, x: Int) -> SIMD[DType.f32, nelts]:\n",
    "        return self.data.simd_load[nelts](y * self.cols + x)\n",
    "\n",
    "    @always_inline\n",
    "    fn load_tr[nelts:Int](self, y: Int, x: Int) -> SIMD[DType.f32, nelts]:\n",
    "        # Perform a transposed simd load. \n",
    "        # return strided_load[nelts,DType.f32](self.data + x* dtype_sizeof[DType.f32](), self.cols)\n",
    "        var res = SIMD[DType.f32, nelts]()\n",
    "        res[0] = self[y + 0, x]\n",
    "        res[1] = self[y + 1, x]\n",
    "        res[2] = self[y + 2, x]\n",
    "        res[3] = self[y + 3, x]\n",
    "        res[4] = self[y + 4, x]\n",
    "        res[5] = self[y + 5, x]\n",
    "        res[6] = self[y + 6, x]\n",
    "        res[7] = self[y + 7, x]\n",
    "        return res\n",
    "\n",
    "    @always_inline\n",
    "    fn __setitem__(self, y: Int, x: Int, val: F32):\n",
    "        return self.store[1](y, x, val)\n",
    "\n",
    "    @always_inline\n",
    "    fn store[nelts:Int](self, y: Int, x: Int, val: SIMD[DType.f32, nelts]):\n",
    "        var data = self.data\n",
    "        data.simd_store[nelts](y * self.cols + x, val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that we implement `getitem` and `setitem` in terms of `load` and `store`. For the naive implementation of matmul it does not make a difference, but we will utilize this later in a more optimized vectorized version of matmul. We are also defining a `load_tr`, which loads a vector from the columns specified at the offset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above `Matrix` type we can effectively copy and paste the Python implementation and just add type annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note that C, A, and B have types.\n",
    "fn matmul_naive(C: Matrix, A: Matrix, B: Matrix):\n",
    "    for m in range(C.rows):\n",
    "        for n in range(C.cols):\n",
    "            for k in range(A.cols):\n",
    "                C[m, n] += A[m, k] * B[k, n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to benchmark the implementations as we improve, so let's write a helper function that will do that for us: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@always_inline\n",
    "def benchmark[func : __mlir_type[\n",
    "    `!kgen.signature<<>(`,\n",
    "    `!pop.pointer<`, Matrix,`>`, # C\n",
    "    ` borrow_in_mem,`,\n",
    "    `!pop.pointer<`, Matrix,`>`, # A\n",
    "    ` borrow_in_mem,`,\n",
    "    `!pop.pointer<`, Matrix,`>`, # B\n",
    "    ` borrow_in_mem) -> `,\n",
    "     NoneType,\n",
    "    `>`,\n",
    "  ]](M : Int, N : Int, K : Int):\n",
    "    var C = Matrix(M, N)\n",
    "    C.zero()\n",
    "    var A = Matrix(M, K)\n",
    "    var B = Matrix(K, N)\n",
    "\n",
    "    # func(C, A, B)\n",
    "    # print(C[10,4])\n",
    "\n",
    "    @always_inline\n",
    "    fn test_fn():\n",
    "        try:\n",
    "            func(C, A, B)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    let secs = F64(Benchmark().run[test_fn]()) / 1_000_000_000\n",
    "    let gflops = ((2*M*N*K)/secs) / 1e9\n",
    "    let speedup : F64 = gflops / python_gflops\n",
    "    print_no_newline(gflops, \"GFLOP/s, a \")\n",
    "    _printf(\"%0.2f\", speedup.value)\n",
    "    print(\"x speedup over Python\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarking shows significant speedups. We increase the size of the matrix to 512 by 512, since Mojo is much faster than Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.702316 GFLOP/s, a 310.62x speedup over Python\n"
     ]
    }
   ],
   "source": [
    " #| CHECK: GFLOP/s\n",
    "benchmark[matmul_naive](512, 512, 512)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding type annotations gives a huge improvement compared to the original untyped version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing the inner most loop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do better than the above implementation by utilizing the vector instructions. Assuming a vector width of 8, we can modify the code to leverage SIMD instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mojo has SIMD vector types, we can vectorize the Matmul code as follows.\n",
    "alias nelts = 8 # The SIMD vector width.\n",
    "fn matmul_vectorized_0(C: Matrix, A: Matrix, B: Matrix):\n",
    "    for m in range(C.rows):\n",
    "        for n in range(C.cols):\n",
    "            var tmp = SIMD[DType.f32, nelts]()\n",
    "            for kv in range(0, A.cols, nelts):\n",
    "                tmp += A.load[nelts](m,kv) * B.load_tr[nelts](kv,n)\n",
    "            C[m,n] += tmp.reduce_add()\n",
    "        \n",
    "            # Handle remaining elements with scalars.\n",
    "            for k in range(nelts*(A.cols//nelts), A.cols):\n",
    "                C[m,n] += A[m,k] * B[k,n]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can benchmark the above implementation. Note that many compilers can detect naive loops and perform optimizations on them. Mojo, however, allows you to be explicit and precisely control what optimizations are applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.136114 GFLOP/s, a 572.25x speedup over Python\n"
     ]
    }
   ],
   "source": [
    " #| CHECK: GFLOP/s\n",
    "benchmark[matmul_vectorized_0](512, 512, 512)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization is a common optimization, and Mojo provides a higher-order function that performs vectorization for you. The `vectorize` function takes a vector width and a function which is parameteric on the vector width and is going to be evaluated in a vectorized manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simplify the code by using the builtin vectorize function\n",
    "from Functional import vectorize\n",
    "fn matmul_vectorized_1(C: Matrix, A: Matrix, B: Matrix):\n",
    "    for m in range(C.rows):\n",
    "        for n in range(C.cols):\n",
    "            fn dot[nelts : Int](k : Int):\n",
    "                C[m,n] += (A.load[nelts](m,k) * B.load_tr[nelts](k,n)).reduce_add()\n",
    "            vectorize[nelts, dot](A.cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is only a slight difference in terms of performance between the two implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.125112 GFLOP/s, a 570.27x speedup over Python\n"
     ]
    }
   ],
   "source": [
    " #| CHECK: GFLOP/s\n",
    "benchmark[matmul_vectorized_1](512, 512, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelizing Matmul"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the best performance from modern processors, one has to utilize the multiple cores they have. With Mojo it can be easily achieved with `parallelize` function.\n",
    "\n",
    "Let's modify our matmul implementation and make it multi-threaded (for simplicity, we only `parallelize` on the M dimension):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parallelize the code by using the builtin parallelize function\n",
    "from Functional import parallelize\n",
    "fn matmul_parallelized(C: Matrix, A: Matrix, B: Matrix):\n",
    "    fn calc_row(m: Int):\n",
    "        for n in range(C.cols):\n",
    "            fn dot[nelts : Int](k : Int):\n",
    "                C[m,n] += (A.load[nelts](m,k) * B.load_tr[nelts](k,n)).reduce_add()\n",
    "            vectorize[nelts, dot](A.cols)\n",
    "        \n",
    "    parallelize[calc_row](C.rows)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can benchmark the parallel matmul implementation. Again, we increase the size of the matrix to 1024 by 1024, since this implementation is much faster than the single-threaded version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.388119 GFLOP/s, a 2078.00x speedup over Python\n"
     ]
    }
   ],
   "source": [
    " #| CHECK: GFLOP/s\n",
    "benchmark[matmul_parallelized](1024, 1024, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiling Matmul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiling is an optimization performed for matmul to increase cache locality. The idea is to keep sub-matrices resident in the cache and increase the reuse. The tile function itself can be written in Mojo as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Functional import Static2DTileUnitFunc as Tile2DFunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform 2D tiling on the iteration space defined by end_x and end_y.\n",
    "fn tile[tiled_fn: Tile2DFunc, tile_x: Int, tile_y: Int](end_x: Int, end_y: Int):\n",
    "    # Note: this assumes that ends are multiples of the tiles.\n",
    "    for y in range(0, end_y, tile_y):\n",
    "        for x in range(0, end_x, tile_x):\n",
    "            tiled_fn[tile_x, tile_y](x, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above will perform 2 dimensional tiling over a 2D iteration space defined to be between $([0, end_x], [0, end_y])$. Once we define it above, we can use it within our matmul kernel. For simplicity we choose `16` as the tile height and since we also want to vectorize we use `16 * nelts` as the tile width (since we vectorize on the columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the above tile function to perform tiled matmul.\n",
    "fn matmul_tiled_parallelized(C: Matrix, A: Matrix, B: Matrix):\n",
    "    fn calc_row(m: Int):\n",
    "        fn calc_tile[tile_x: Int, tile_y: Int](x: Int, y: Int):\n",
    "            for n in range(y, y + tile_y):\n",
    "                fn dot[nelts: Int](k: Int):\n",
    "                    C[m,n] += (A.load[nelts](m,k+x) * B.load_tr[nelts](k+x,n)).reduce_add()\n",
    "                vectorize[nelts, dot](tile_x)\n",
    "        \n",
    "        # We hardcode the tile factor to be 16.\n",
    "        alias tile_size = 16\n",
    "        tile[calc_tile, nelts * tile_size, tile_size](C.cols, A.cols)\n",
    "\n",
    "    parallelize[calc_row](C.rows)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can benchmark the tiled parallel matmul implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.896004 GFLOP/s, a 2170.67x speedup over Python\n"
     ]
    }
   ],
   "source": [
    " #| CHECK: GFLOP/s\n",
    "benchmark[matmul_tiled_parallelized](1024, 1024, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One source of overhead in the above implementation is the fact that the we are not unrolling the loops introduced by vectorize of the dot function. We can do that via the `vectorize_unroll` higher-order function in Mojo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Unroll the vectorized loop by a constant factor.\n",
    "from Functional import vectorize_unroll\n",
    "fn matmul_tiled_unrolled_parallelized(C: Matrix, A: Matrix, B: Matrix):\n",
    "    fn calc_row(m: Int):\n",
    "        fn calc_tile[tile_x: Int, tile_y: Int](x: Int, y: Int):\n",
    "            for n in range(y, y + tile_y):\n",
    "                fn dot[nelts : Int](k : Int):\n",
    "                    C[m,n] += (A.load[nelts](m,k+x) * B.load_tr[nelts](k+x,y)).reduce_add()\n",
    "            \n",
    "                # Vectorize by nelts and unroll by tile_x/nelts\n",
    "                # Here unroll factor is 16/8 = 2\n",
    "                vectorize_unroll[nelts, tile_x//nelts, dot](tile_x)\n",
    "\n",
    "        alias tile_size = 16\n",
    "        tile[calc_tile, nelts*tile_size, tile_size](A.cols, C.cols)\n",
    "      \n",
    "    parallelize[calc_row](C.rows)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can benchmark the new tiled parallel matmul implementation with unrolled and vectorized inner loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.243320 GFLOP/s, a 2234.05x speedup over Python\n"
     ]
    }
   ],
   "source": [
    " #| CHECK: GFLOP/s\n",
    "benchmark[matmul_tiled_unrolled_parallelized](1024, 1024, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for the `tile_factor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Autotune import autotune, search\n",
    "from Time import now\n",
    "from Pointer import Pointer\n",
    "\n",
    "alias matmul_fn_type = __mlir_type[\n",
    "    `( `,\n",
    "    Pointer[Matrix].pointer_type,\n",
    "    `, `,\n",
    "    Pointer[Matrix].pointer_type,\n",
    "    `, `,\n",
    "    Pointer[Matrix].pointer_type,\n",
    "    `) -> `, NoneType\n",
    "]\n",
    "alias matmul_fn_sig_type = __mlir_type[\n",
    "    `!kgen.signature<<>(`,\n",
    "    `!pop.pointer<`, Matrix,`>`, # C\n",
    "    ` borrow_in_mem,`,\n",
    "    `!pop.pointer<`, Matrix,`>`, # A\n",
    "    ` borrow_in_mem,`,\n",
    "    `!pop.pointer<`, Matrix,`>`, # B\n",
    "    ` borrow_in_mem) -> `, NoneType,\n",
    "    `>`,\n",
    "  ]\n",
    "\n",
    "alias matmul_fn_ptr_type = __mlir_type[`!pop.pointer<`, matmul_fn_sig_type, `>`]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of the tile factor can greatly impact the performace of the full matmul,\n",
    "but the optimal tile factor is highly hardware-dependent, and is influenced by the\n",
    "cache configuration and other hard-to-model effects. We want write to write portable code\n",
    "without having to know everything about the hardware, so we can ask Mojo to automatically\n",
    "select the best tile factor using autotuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Autotune the tile size used in the matmul.\n",
    "@adaptive\n",
    "fn matmul_autotune_impl(C: Matrix, A: Matrix, B: Matrix):\n",
    "    fn calc_row(m: Int):\n",
    "        fn calc_tile[tile_x: Int, tile_y: Int](x: Int, y: Int):\n",
    "            for n in range(y, y + tile_y):\n",
    "                fn dot[nelts : Int](k : Int):\n",
    "                    C[m,n] += (A.load[nelts](m,k+x) * B.load_tr[nelts](k+x,y)).reduce_add()\n",
    "                vectorize_unroll[nelts, tile_x // nelts, dot](tile_x)\n",
    "\n",
    "        # Instead of hardcoding to tile_size = 16, search for the fastest \n",
    "        # tile size by evaluting this function as tile size varies.\n",
    "        alias tile_size = autotune(1, 2, 4, 8, 16, 32, 64)\n",
    "        tile[calc_tile, nelts * tile_size, tile_size](A.cols, C.cols)\n",
    "      \n",
    "    parallelize[calc_row](C.rows)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will generate multiple candidates for the matmul function. To teach Mojo how\n",
    "to find the best tile factor, we provide an evaluator function Mojo can use to\n",
    "assess each candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fn matmul_evaluator(funcs: matmul_fn_ptr_type, size: Int) -> Int:\n",
    "    print(\"matmul_evaluator, number of candidates: \")\n",
    "    print(size)\n",
    "\n",
    "    let eval_begin: Int = now()\n",
    "\n",
    "    # This size is picked at random, in real code we could use a real size\n",
    "    # distribution here.\n",
    "    let M = 1024\n",
    "    let N = 1024\n",
    "    let K = 1024\n",
    "    print(\"Optimizing for size:\", M, \"x\", N, \"x\", K)\n",
    "\n",
    "    var best_idx: Int = -1\n",
    "    var best_time: Int = -1\n",
    "    var funcs_ptr = Pointer[matmul_fn_sig_type](funcs).bitcast[matmul_fn_type]()\n",
    "\n",
    "    alias eval_iterations = 10\n",
    "    alias eval_samples = 10\n",
    "\n",
    "    var C = Matrix(M, N)\n",
    "    var A = Matrix(M, K)\n",
    "    var B = Matrix(K, N)\n",
    "    let Cptr = Pointer[Matrix].address_of(C).address\n",
    "    let Aptr = Pointer[Matrix].address_of(A).address\n",
    "    let Bptr = Pointer[Matrix].address_of(B).address\n",
    "\n",
    "    # Find the function that's the fastest on the size we're optimizing for\n",
    "    for f_idx in range(size):\n",
    "        let func = funcs_ptr.load(f_idx)\n",
    "\n",
    "        @always_inline\n",
    "        fn wrapper():\n",
    "            __mlir_op.`pop.call_indirect`[_type:NoneType](\n",
    "                func, Cptr, Aptr, Bptr\n",
    "            )\n",
    "        let cur_time = Benchmark(1, 100_000, 500_000_000, 1000_000_000).run[wrapper]()\n",
    "\n",
    "        if best_idx < 0:\n",
    "            best_idx = f_idx\n",
    "            best_time = cur_time\n",
    "        if best_time > cur_time:\n",
    "            best_idx = f_idx\n",
    "            best_time = cur_time\n",
    "\n",
    "    let eval_end: Int = now()\n",
    "    print(\"Time spent in matmul_evaluator, ms:\", (eval_end - eval_begin) // 1000000)\n",
    "    print(\"Best candidate idx:\", best_idx)\n",
    "    return best_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to define an entry function that would simply call the best candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fn matmul_autotune(C: Matrix, A: Matrix, B: Matrix):\n",
    "    alias best_impl: matmul_fn_sig_type\n",
    "    search[\n",
    "        matmul_fn_sig_type,\n",
    "        VariadicList(matmul_autotune_impl.__adaptive_set),\n",
    "        matmul_evaluator -> best_impl\n",
    "    ]()\n",
    "    # Run the best candidate\n",
    "    return best_impl(C, A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's benchmark our new implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.824017 GFLOP/s, a 4164.72x speedup over Python\n"
     ]
    }
   ],
   "source": [
    " #| CHECK: GFLOP/s\n",
    "benchmark[matmul_autotune](1024, 1024, 1024)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mojo",
   "language": "mojo",
   "name": "mojo-jupyter-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "mojo"
   },
   "file_extension": ".mojo",
   "mimetype": "text/x-mojo",
   "name": "mojo"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
